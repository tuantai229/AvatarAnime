# Diffusion Models và Stable Diffusion (Vi)

## Diffusion Models: Nguyên lý cơ bản

Diffusion Models là một họ mô hình sinh đang dẫn đầu trong lĩnh vực tạo hình ảnh, sử dụng cách tiếp cận khác biệt so với GAN.

### Quá trình Diffusion

Diffusion Models hoạt động dựa trên hai quá trình:

1. **Forward Diffusion Process (Quá trình khuếch tán thuận)**:
   - Dần dần thêm nhiễu Gaussian vào dữ liệu gốc qua T bước
   - Ở bước cuối, dữ liệu gốc biến thành nhiễu Gaussian hoàn toàn
   - Công thức: x_t = √(1-β_t) * x_{t-1} + √(β_t) * ε

2. **Reverse Diffusion Process (Quá trình khuếch tán ngược)**:
   - Học cách loại bỏ nhiễu, dần dần khôi phục dữ liệu gốc
   - Bắt đầu từ nhiễu hoàn toàn, mô hình dự đoán và loại bỏ nhiễu ở mỗi bước
   - Công thức cơ bản: x_{t-1} = f(x_t, t, θ) + σ_t * z

### Mục tiêu học tập

Diffusion Models tập trung vào việc học dự đoán nhiễu đã thêm vào:
- Trong quá trình huấn luyện, mô hình học cách dự đoán ε từ x_t
- Mục tiêu: Tối thiểu hóa MSE giữa nhiễu thực tế và nhiễu dự đoán

Hàm loss cơ bản:
```
L = E_{x,ε,t}[||ε - ε_θ(x_t, t)||²]
```

### Ưu điểm của Diffusion Models

1. **Chất lượng cao**: Tạo ra hình ảnh chất lượng cao và đa dạng hơn GANs
2. **Ổn định trong huấn luyện**: Không đối mặt với vấn đề mode collapse hay huấn luyện bất ổn định
3. **Khả năng điều khiển tốt hơn**: Cho phép điều khiển chi tiết hơn thông qua guidance và điều kiện
4. **Dễ đánh giá**: Có thể tính likelihood của mô hình

## Stable Diffusion

Stable Diffusion là một mô hình diffusion tiên tiến được phát triển bởi CompVis, Stability AI và LAION.

### Kiến trúc của Stable Diffusion

Điểm khác biệt chính của Stable Diffusion là thực hiện quá trình diffusion trong không gian tiềm ẩn (latent space) thay vì trực tiếp trên hình ảnh:

1. **Encoder (VAE encoder)**: Nén hình ảnh thành biểu diễn latent space
2. **U-Net với attention**: Thực hiện quá trình diffusion trong không gian latent
3. **Decoder (VAE decoder)**: Giải nén từ latent space về không gian hình ảnh

### Conditional Generation trong Stable Diffusion

Stable Diffusion cho phép điều kiện hóa việc tạo ảnh thông qua:

1. **Text-to-Image**: Sử dụng embedding từ CLIP text encoder
2. **Image-to-Image**: Bắt đầu từ một ảnh đã có và thực hiện diffusion
3. **Inpainting/Outpainting**: Chỉ thay đổi một phần của ảnh

### Các khái niệm quan trọng

1. **Noise Scheduler**: Kiểm soát lịch trình thêm/bớt nhiễu (DDPM, DDIM, ...)
2. **Classifier-Free Guidance**: Kỹ thuật để tăng cường việc tuân theo điều kiện
3. **Negative Prompts**: Hướng dẫn mô hình về những gì KHÔNG nên tạo ra

## ControlNet cho Stable Diffusion

ControlNet là một mở rộng quan trọng cho Stable Diffusion, đặc biệt hữu ích cho dự án chuyển đổi ảnh người sang anime.

### Nguyên lý hoạt động của ControlNet

- Thêm các đường dẫn điều khiển (control paths) vào mô hình Stable Diffusion
- Cho phép điều khiển chi tiết quá trình tạo ảnh dựa trên các điều kiện như cấu trúc, đường viền, pose, v.v.
- Tận dụng kiến trúc "zero convolution" để đảm bảo không gây nhiễu quá trình huấn luyện

### Các loại điều kiện trong ControlNet

1. **Canny Edge**: Điều khiển bằng đường viền
2. **HED Edge**: Điều khiển bằng đường viền với nhiều chi tiết hơn
3. **Pose/OpenPose**: Điều khiển bằng khung xương người
4. **Depth Map**: Điều khiển bằng bản đồ độ sâu
5. **Normal Map**: Điều khiển bằng bản đồ pháp tuyến
6. **Segmentation Map**: Điều khiển bằng bản đồ phân đoạn

### Áp dụng ControlNet cho chuyển đổi người thật sang anime

ControlNet là lựa chọn tuyệt vời cho dự án này vì nó cho phép:
1. Bảo toàn đặc điểm nhận dạng (thông qua điều khiển cấu trúc)
2. Duy trì tư thế, biểu cảm từ ảnh gốc
3. Áp dụng phong cách anime một cách nhất quán

## Fine-tuning Stable Diffusion

### LoRA (Low-Rank Adaptation)

LoRA là phương pháp hiệu quả để fine-tune Stable Diffusion mà không cần cập nhật toàn bộ mô hình:

1. **Nguyên lý**: Thêm các ma trận rank thấp vào các weight hiện có
2. **Ưu điểm**: 
   - Yêu cầu ít tài nguyên hơn so với fine-tuning toàn bộ
   - Có thể kết hợp nhiều LoRA khác nhau
   - Dễ dàng chuyển đổi giữa các phong cách

### Textual Inversion

Phương pháp học các token mới để biểu diễn phong cách hoặc đối tượng cụ thể:

1. **Nguyên lý**: Học embedding cho "từ" mới đại diện cho khái niệm cụ thể
2. **Ứng dụng**: Học một token đại diện cho phong cách anime cụ thể

### DreamBooth

Phương pháp fine-tune để mô hình học về một đối tượng cụ thể:

1. **Nguyên lý**: Fine-tune toàn bộ mô hình với một số lượng ảnh nhỏ của đối tượng
2. **Ứng dụng**: Có thể áp dụng để học cách bảo toàn đặc điểm nhận dạng của một người cụ thể

## Triển khai Stable Diffusion với ControlNet

### Cấu hình môi trường

```python
# Cài đặt các thư viện cần thiết
!pip install diffusers transformers accelerate ftfy safetensors
!pip install opencv-python controlnet_aux
```

### Mô hình cơ bản cho Image-to-Image với ControlNet

```python
import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image
import numpy as np
import cv2
from PIL import Image

# Tải mô hình ControlNet (ví dụ: sử dụng Canny Edge)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16
)

# Tải mô hình Stable Diffusion với ControlNet
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "animefull-final-pruned", # Hoặc mô hình anime khác
    controlnet=controlnet,
    torch_dtype=torch.float16,
).to("cuda")

# Chuẩn bị ảnh điều khiển (từ ảnh người thật)
image = load_image("path/to/person.jpg")
image = np.array(image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
control_image = Image.fromarray(image)

# Tạo ảnh anime
prompt = "high quality anime portrait, detailed anime face, anime style, beautiful anime girl"
negative_prompt = "low quality, bad anatomy, worst quality, blurry"

output = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=control_image,
    num_inference_steps=30,
    guidance_scale=7.5,
).images[0]

output.save("anime_portrait.png")
```

### Kết hợp nhiều điều kiện ControlNet

Để kết quả tốt hơn, có thể kết hợp nhiều loại ControlNet:

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, MultiControlNetModel

# Tải nhiều mô hình ControlNet
canny_controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
pose_controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-openpose", torch_dtype=torch.float16)

# Kết hợp các ControlNet
controlnet = MultiControlNetModel([canny_controlnet, pose_controlnet])

# Tạo pipeline
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "anime-model", controlnet=controlnet, torch_dtype=torch.float16
).to("cuda")

# Chuẩn bị ảnh điều khiển
canny_image = # Xử lý canny từ ảnh gốc
pose_image = # Xử lý pose từ ảnh gốc

# Tạo ảnh
output = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=[canny_image, pose_image],
    num_inference_steps=30,
    guidance_scale=7.5,
    control_guidance_start=[0.0, 0.0],
    control_guidance_end=[1.0, 1.0],
).images[0]
```

## Fine-tune Stable Diffusion cho Anime Style

### Sử dụng LoRA

```python
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
import torch

# Tải mô hình Stable Diffusion cơ bản
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")

# Tải LoRA adapter cho anime style
pipe.unet.load_attn_procs(
    "anime-style-lora",
    weight_name="anime_style_lora.safetensors"
)

# Cấu hình scheduler
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

# Sử dụng cho image-to-image
pipe = StableDiffusionImg2ImgPipeline(**pipe.components)

# Tạo ảnh anime từ ảnh người thật
init_image = load_image("path/to/person.jpg").resize((512, 512))
prompt = "anime style, highly detailed anime character, anime portrait"

image = pipe(
    prompt=prompt,
    image=init_image,
    strength=0.75,  # Mức độ thay đổi (0-1)
    guidance_scale=7.5,
    num_inference_steps=50
).images[0]
```

## Thách thức và Giải pháp

### Thách thức khi sử dụng Stable Diffusion

1. **Bảo toàn đặc điểm nhận dạng**: Khó đảm bảo người trong ảnh vẫn có thể nhận ra
   - **Giải pháp**: Sử dụng ControlNet + kết hợp với DreamBooth/LoRA được fine-tune cho người cụ thể

2. **Ổn định phong cách**: Đảm bảo phong cách anime nhất quán
   - **Giải pháp**: Fine-tune trên dataset anime với phong cách cụ thể

3. **Đa dạng phong cách**: Tạo nhiều phong cách anime khác nhau
   - **Giải pháp**: Huấn luyện nhiều LoRA khác nhau cho từng phong cách

4. **Tính toán nặng nề**: Stable Diffusion yêu cầu GPU mạnh
   - **Giải pháp**: Sử dụng các kỹ thuật như model pruning, quantization, hoặc thuê GPU cloud