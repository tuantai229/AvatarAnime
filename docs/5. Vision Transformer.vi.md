# Giai đoạn 5: Triển khai hướng tiếp cận 3 - Vision Transformer (Vi)

`src/models/vit/ViT_StyleTransfer_Kaggle.ipynb` 

## 1. Mục tiêu

Sử dụng Vision Transformer (ViT) kết hợp với kỹ thuật Adaptive Instance Normalization (AdaIN) để chuyển đổi ảnh chân dung người thật (content image) sang phong cách hoạt hình anime (style image). Các yêu cầu chính bao gồm bảo toàn đặc điểm nhận dạng của người trong ảnh gốc và tạo ra kết quả có chất lượng cao, mang đặc trưng rõ ràng của phong cách anime.

## 2. Môi trường Thực thi

* **Nền tảng:** Kaggle Notebook
* **Phần cứng:** 2 x GPU NVIDIA Tesla T4
* **Ngôn ngữ:** Python 3.11
* **Thư viện chính:**
    * `torch` & `torchvision`: Framework học sâu.
    * `timm`: Thư viện chứa các mô hình pre-trained (bao gồm ViT).
    * `PIL` (Pillow): Xử lý ảnh.
    * `matplotlib`: Vẽ đồ thị và hiển thị ảnh.
    * `numpy`: Tính toán số học.
    * `tqdm`: Thanh tiến trình.
    * `os`, `shutil`: Thao tác với hệ thống file.

## 3. Dữ liệu

* **Nguồn:** Kaggle Dataset - [AvatarAnime](https://www.kaggle.com/datasets/tuantai229/avataranime)
* **Đường dẫn trên Kaggle:** `/kaggle/input/avataranime/AvatarAnime/`
    * Ảnh Content (Người thật): `CelebA/train`, `CelebA/val`, `CelebA/test`
    * Ảnh Style (Anime): `AnimeFace/train`, `AnimeFace/val`, `AnimeFace/test`
* **Số lượng (trong notebook):**
    * Train: 800 content, 800 style
    * Validation: 100 content, 100 style
* **Tiền xử lý & Augmentation:**
    * **Kích thước ảnh (`IMG_SIZE`):** 224x224
    * **Chuẩn hóa:** Sử dụng giá trị mean và std của ImageNet.
    * **Train Transform:** Resize(224), RandomCrop(224), RandomHorizontalFlip, ToTensor, Normalize.
    * **Test/Validation Transform:** Resize(224), CenterCrop(224), ToTensor, Normalize.
    * **Hàm `denormalize`:** Được định nghĩa để chuyển ảnh tensor đã chuẩn hóa về dạng xem được.
* **Dataset Class:** Sử dụng lớp `CustomImageDataset` tự định nghĩa để đọc ảnh trực tiếp từ các thư mục `train`, `val`, `test` do dữ liệu không có cấu trúc thư mục con theo lớp.
* **DataLoaders:**
    * `BATCH_SIZE = 8` (Lưu ý: Khi chạy DataParallel, batch size hiệu dụng trên mỗi GPU là 4).
    * `NUM_WORKERS = 2`.
    * `shuffle=True`, `drop_last=True` cho DataLoader huấn luyện.

## 4. Kiến trúc Mô hình: ViTStyleTransfer

Mô hình chính dựa trên kiến trúc Encoder-AdaIN-Decoder.

* **Encoder (`vit_encoder`):**
    * Sử dụng `timm.create_model('vit_base_patch16_224', pretrained=True)`.
    * Lớp `head` (classification) được thay thế bằng `nn.Identity()`.
    * Trích xuất các patch embeddings sau các block Transformer và LayerNorm cuối.
    * Reshape output thành dạng feature map không gian: `(B, embed_dim, 14, 14)`.
* **Adaptive Instance Normalization (`adaptive_instance_normalization`):**
    * Một hàm riêng biệt nhận feature map của content và style.
    * Tính mean và std dọc theo chiều không gian cho từng feature map.
    * Chuẩn hóa content features (trừ mean, chia std).
    * Scale và shift content features đã chuẩn hóa bằng std và mean của style features.
* **Decoder (`Decoder`):**
    * Một mạng CNN sử dụng `nn.ConvTranspose2d` để tăng độ phân giải.
    * Kiến trúc: 4 lớp ConvTranspose2d (stride=2) để upsample từ 14x14 lên 224x224, xen kẽ với `nn.InstanceNorm2d` và `nn.ReLU` (đảm bảo `inplace=False`).
    * Lớp cuối: `nn.Conv2d` (kernel=3, stride=1, padding=1) để chuyển đổi số kênh thành 3 (RGB) mà không thay đổi kích thước 224x224.
    * Lớp kích hoạt cuối: `nn.Tanh()` để đưa output về khoảng [-1, 1].
* **Mạng Tính Loss (`vgg_loss_net`):**
    * Sử dụng `vgg19` pre-trained từ `torchvision.models` (`VGG19_Weights.DEFAULT`).
    * Toàn bộ mạng VGG được đóng băng (`requires_grad=False`) và đặt ở chế độ `eval()`.
    * Các features được trích xuất từ các lớp ReLU cụ thể (indices `[1, 6, 11, 20, 29]`) để tính toán content và style loss.
* **Quy trình `forward`:**
    1.  Encode ảnh content và style bằng `vit_encoder`.
    2.  Áp dụng `adaptive_instance_normalization` lên features.
    3.  Decode features đã được style hóa bằng `Decoder` để tạo ảnh output.
* **Hàm `calculate_loss`:**
    * Trích xuất features VGG từ ảnh generated, content, và style. Features từ ảnh content/style gốc được lấy trong ngữ cảnh `torch.no_grad()`.
    * **Content Loss:** Tính `MSELoss` giữa features VGG (từ lớp `relu3_1`?, index=2) của ảnh generated và ảnh content.
    * **Style Loss:** Tính `MSELoss` trên *mean* và *std* của features VGG (từ tất cả các lớp đã chọn) giữa ảnh generated và ảnh style.
    * Trả về `content_loss` và `style_loss` riêng biệt.

## 5. Huấn luyện

* **Thiết bị:** Chạy trên 2 GPU Tesla T4 sử dụng `torch.nn.DataParallel`.
    * Model gốc được khởi tạo và sau đó được bọc bởi `nn.DataParallel(base_model)`.
* **Optimizer:** `AdamW` với `lr=1e-4`.
* **Hyperparameters:**
    * `NUM_EPOCHS = 50`
    * `LAMBDA_CONTENT = 3.0`
    * `LAMBDA_STYLE = 1.0` (Tỉ lệ này ưu tiên giữ nội dung hơn là áp dụng style mạnh)
* **Vòng lặp Huấn luyện:**
    1.  Đặt model ở chế độ `train()`.
    2.  Lặp qua các batch từ `content_loader_train` và `style_loader_train`.
    3.  Chuyển dữ liệu lên device (`cuda`).
    4.  Xóa gradient (`optimizer.zero_grad()`).
    5.  Forward pass qua model để tạo `generated_images`.
    6.  Tính loss: Gọi `model.module.calculate_loss(...)` để truy cập hàm của model gốc khi dùng `DataParallel`.
    7.  Tính `total_loss = LAMBDA_CONTENT * content_loss + LAMBDA_STYLE * style_loss`.
    8.  Kiểm tra `NaN` trong loss.
    9.  Tính gradient (`total_loss.backward()`).
    10. Cập nhật trọng số (`optimizer.step()`).
    11. Ghi log loss trung bình mỗi epoch.
* **Validation:**
    * Sau mỗi epoch, đặt model ở chế độ `eval()`.
    * Chạy inference trên một batch cố định từ tập validation.
    * Denormalize và lưu ảnh kết quả (content, style, generated) dưới dạng grid bằng `matplotlib`.
* **Lưu Checkpoint:**
    * Lưu checkpoint mỗi 5 epoch vào thư mục `/kaggle/working/ViT_StyleTransfer/checkpoints/`.
    * **Quan trọng:** Checkpoint được lưu bằng `model.state_dict()`. Do `model` là đối tượng `DataParallel`, `state_dict` này chứa các key có tiền tố `module.`.

## 6. Kết quả Huấn luyện (Dựa trên Log và Ảnh Inference)

* **Loss:** Giá trị loss (Total, Content, Style) giảm dần qua 50 epochs, cho thấy mô hình có học. Loss cuối cùng (Epoch 50) có giá trị Total ~8.09, Content ~1.42, Style ~3.82. Tỉ lệ `LAMBDA_CONTENT=3.0`, `LAMBDA_STYLE=1.0` có thể giải thích tại sao Content Loss tương đối thấp.
* **Chất lượng Ảnh:**
    * **Tiến triển:** Từ nhiễu ở epoch 1, mô hình nhanh chóng học cách giữ lại cấu trúc khuôn mặt của ảnh content (từ epoch 6 trở đi).
    * **Kết quả cuối (Epoch 50):** Ảnh generated giữ được khá tốt nội dung và hình dáng của ảnh gốc. Tuy nhiên, hiệu ứng phong cách anime còn yếu, chủ yếu thể hiện qua sự thay đổi về màu sắc và độ mịn tổng thể. Các đặc trưng anime như mắt to, đường nét sắc cạnh, kiểu tô màu đặc trưng chưa rõ ràng. Ảnh trông giống ảnh gốc được áp filter hơn là chuyển đổi style hoàn chỉnh.
    * **Hội tụ:** Chất lượng dường như không cải thiện nhiều sau khoảng epoch 15-20, cho thấy mô hình có thể đã hội tụ với các tham số hiện tại hoặc cần điều chỉnh loss để đẩy mạnh việc học style.

## 7. Inference (Sử dụng mô hình đã huấn luyện)

* Hàm `run_inference` được định nghĩa để tải checkpoint và thực hiện chuyển đổi style cho một cặp ảnh content/style mới.
* **Tải Checkpoint:**
    * Sử dụng `torch.load(..., weights_only=True)` để tải checkpoint.
    * Khởi tạo model gốc `ViTStyleTransfer` (không có `DataParallel`).
    * **Xử lý `state_dict`:** Code trong hàm `run_inference` đã được điều chỉnh để kiểm tra và loại bỏ tiền tố `module.` khỏi các key trong `state_dict` tải từ checkpoint trước khi load vào model gốc. Điều này là cần thiết do checkpoint được lưu từ đối tượng `DataParallel`.
* **Quy trình:** Load ảnh -> Transform -> Chuyển lên device -> Chạy model (`inference_model(...)` trong `torch.no_grad()`) -> Denormalize -> Lưu ảnh kết quả.
* Notebook đã chạy inference thành công cho các checkpoint được lưu mỗi 5 epoch.

## 8. Thảo luận và Hướng phát triển Tiếp theo

* **Thành công:**
    * Khắc phục được lỗi noise ban đầu và các lỗi runtime (inplace, attribute error).
    * Huấn luyện thành công mô hình trên đa GPU (2x T4) bằng `DataParallel`.
    * Mô hình học được cách bảo toàn nội dung ảnh gốc khá tốt.
* **Hạn chế:**
    * Chất lượng chuyển đổi phong cách anime chưa đạt yêu cầu (style yếu).
* **Dự định cải tiến:**
    * **Tinh chỉnh Loss:** Thử nghiệm mạnh mẽ với việc tăng `LAMBDA_STYLE` và/hoặc giảm `LAMBDA_CONTENT`.
    * **Phương pháp Style Loss:** Chuyển sang dùng Gram Matrix thay vì mean/std.
    * **Lớp VGG:** Thử nghiệm các lớp VGG khác nhau cho content và style loss.
    * **TV Loss:** Thêm Total Variation Loss để làm mượt ảnh.
    * **Dữ liệu:** Kiểm tra chất lượng và tính nhất quán của tập AnimeFace.
    * **Hyperparameters:** Thử learning rate thấp hơn, sử dụng scheduler.
    * **Thời gian Huấn luyện:** Huấn luyện thêm epochs sau khi điều chỉnh.