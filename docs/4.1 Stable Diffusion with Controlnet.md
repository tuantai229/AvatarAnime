# Stable Diffusion with ControlNet

`src/models/diffusion/Stable_Diffusion_with_Controlnet_Colab.ipynb` 

## 1. Introduction

Stable Diffusion combined with ControlNet is a powerful approach for the human-to-anime image conversion problem. This implementation allows detailed control of the image generation process based on features of the original image, helping preserve identity features - one of the biggest challenges of the project.

### 1.1 What is ControlNet?

ControlNet is an extension of Stable Diffusion, enabling control of the image generation process based on conditions such as edges, depth maps, pose, or semantic segmentation maps. This is particularly useful for style transfer because it helps maintain the original image structure while changing the visual style.

### 1.2 Benefits of This Approach

- **Identity preservation**: ControlNet helps maintain facial structure, pose, and important features of the original image
- **Style diversity**: Can easily adjust prompts to create various anime styles
- **High quality**: Results typically have better resolution and detail than CycleGAN
- **Flexibility**: Can use different types of control conditions (canny, depth, pose, etc.)

## 2. Environment Setup

To implement Stable Diffusion with ControlNet, we need to install the necessary libraries:

```python
!pip install diffusers transformers accelerate PilloW opencv-python-headless controlnet_aux matplotlib
```

### 2.1 Import Libraries

```python
import torch
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from controlnet_aux import CannyDetector  # Using Canny edge as example
from PIL import Image
import numpy as np
import os
import random
import matplotlib.pyplot as plt
```

### 2.2 Configure Device

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")
```

## 3. Loading ControlNet and Stable Diffusion Models

### 3.1 Load ControlNet (Canny)

```python
controlnet_id = "lllyasviel/sd-controlnet-canny"
print(f"Loading ControlNet model: {controlnet_id}")
controlnet = ControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16)
print("ControlNet model loaded.")
```

> **Note**: You can try other types of ControlNet such as:
> - `lllyasviel/sd-controlnet-openpose`: For control based on human pose
> - `lllyasviel/sd-controlnet-depth`: For control based on depth information

### 3.2 Load Stable Diffusion Pipeline

```python
stable_diffusion_id = "runwayml/stable-diffusion-v1-5"
print(f"Loading Stable Diffusion pipeline with ControlNet support: {stable_diffusion_id}")

# Load Img2Img Pipeline with ControlNet support
pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    stable_diffusion_id,
    controlnet=controlnet,
    torch_dtype=torch.float16
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_attention_slicing()
pipe.to(device)
print("Img2Img Pipeline ready.")
```

> **Note**: You can experiment with other Stable Diffusion checkpoints such as:
> - `emilianJR/AnythinV4.0`: Fine-tuned for anime style
> - `hakurei/waifu-diffusion`: Specialized for anime/manga

### 3.3 Load Canny Edge Processor

```python
canny_detector = CannyDetector()
print("Canny detector ready.")
```

## 4. Input Image Processing

### 4.1 Read Image and Create Canny Edge Map

```python
# Read original image
target_size = 512  # Good size for Stable Diffusion
input_image_pil = load_image(input_image_path)
input_image_pil = input_image_pil.resize((target_size, target_size))

# Convert PIL Image to Numpy array
input_image_np = np.array(input_image_pil)

# Create Canny Edge Map
low_threshold = 100
high_threshold = 200
canny_image_np = canny_detector(input_image_np, low_threshold, high_threshold)
canny_image_pil = Image.fromarray(canny_image_np)
```

### 4.2 Display Original Image and Canny Map

```python
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(input_image_pil)
axes[0].set_title('Original Image (Resized)')
axes[0].axis('off')

axes[1].imshow(canny_image_pil, cmap='gray')
axes[1].set_title('Canny Edges Map')
axes[1].axis('off')
plt.show()
```

## 5. Generate Image with Img2Img ControlNet

### 5.1 Define Prompt and Parameters

```python
prompt = "anime style portrait, masterpiece, best quality, sharp focus, detailed illustration, 1boy"
negative_prompt = "photorealistic, real photo, low quality, blurry, noisy, text, watermark, signature, ugly, deformed"

generator_seed = random.randint(0, 1000000)
generator = torch.Generator(device=device).manual_seed(generator_seed)

num_inference_steps = 30
guidance_scale = 9.0
controlnet_conditioning_scale = 0.8

# Important parameter for Img2Img
image_strength = 0.5  # Adjust this value to change conversion intensity (0.0-1.0)
```

### 5.2 Run Img2Img Pipeline with ControlNet

```python
output_image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=input_image_pil,            # Original image
    control_image=canny_image_pil,    # Canny Edge map
    strength=image_strength,          # Degree of change to original image
    num_inference_steps=num_inference_steps,
    generator=generator,
    guidance_scale=guidance_scale,
    controlnet_conditioning_scale=controlnet_conditioning_scale,
).images[0]
```

### 5.3 Display Results

```python
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(input_image_pil)
axes[0].set_title('Original Image (Resized)')
axes[0].axis('off')

axes[1].imshow(canny_image_pil, cmap='gray')
axes[1].set_title('Canny Edges Map')
axes[1].axis('off')

axes[2].imshow(output_image)
axes[2].set_title('Result Image (Anime Style)')
axes[2].axis('off')

plt.show()
```