# Nghiên cứu về Vision Transformers và các mô hình dựa trên Transformer (Vi)

## Transformer: Nền tảng cơ bản

Trước khi đi vào Vision Transformer, hãy hiểu cơ chế cốt lõi của Transformer:

### Kiến trúc Transformer

Transformer được giới thiệu trong bài báo "Attention is All You Need" (2017), bao gồm:

1. **Self-Attention**: Cơ chế cho phép mô hình tập trung vào các phần quan trọng của dữ liệu đầu vào
2. **Multi-Head Attention**: Nhiều cơ chế attention hoạt động song song
3. **Feed-Forward Networks**: Xử lý thông tin sau attention
4. **Residual Connections và Layer Normalization**: Giúp ổn định quá trình huấn luyện

### Self-Attention Mechanism

Cơ chế chính làm nên sức mạnh của Transformer:

- Tính toán ma trận Query (Q), Key (K), và Value (V) từ đầu vào
- Tính điểm tương đồng giữa Q và K: `Attention(Q, K, V) = softmax(QK^T/√d_k)V`
- Cho phép mô hình học các mối quan hệ phụ thuộc dài hạn hiệu quả hơn các mô hình như RNN/LSTM

## Vision Transformer (ViT)

### Từ NLP đến Computer Vision

Vision Transformer (ViT) áp dụng kiến trúc Transformer cho xử lý hình ảnh, được giới thiệu trong bài báo "An Image is Worth 16x16 Words" (2020).

### Kiến trúc của Vision Transformer

1. **Patch Embedding**:
   - Chia ảnh thành các patch (thường 16×16 pixels)
   - Chiếu mỗi patch thành vector embedding

2. **Position Embedding**:
   - Thêm thông tin vị trí vào các patch embedding
   - Có thể là learned position embedding hoặc fixed position embedding

3. **Class Token**:
   - Thêm token đặc biệt (CLS) vào đầu chuỗi
   - Embedding của token này được sử dụng cho classification

4. **Transformer Encoder**:
   - Xử lý chuỗi các patch embedding qua nhiều lớp Transformer Encoder
   - Mỗi lớp bao gồm Multi-Head Self-Attention và MLP

5. **MLP Head**:
   - Chuyển đổi embedding của CLS token thành dự đoán cuối cùng

### Ưu điểm của ViT

1. **Tầm nhìn toàn cục**: Có thể nắm bắt mối quan hệ giữa các vùng xa nhau trong ảnh
2. **Khả năng mở rộng**: Hiệu suất tăng khi huấn luyện trên dữ liệu lớn
3. **Tính linh hoạt**: Dễ dàng kết hợp với các kỹ thuật khác
4. **Transfer learning**: Hiệu quả khi pre-train trên dataset lớn và fine-tune

### Hạn chế của ViT

1. **Cần nhiều dữ liệu**: Hiệu suất kém trên tập dữ liệu nhỏ nếu không pre-train
2. **Tính toán nặng nề**: Độ phức tạp O(n²) với n là số patch
3. **Thiếu tính bất biến với translation**: Không có tính chất inductive bias như CNN

## Các biến thể của Vision Transformer

### DeiT (Data-efficient Image Transformer)

- Cải thiện hiệu quả huấn luyện khi có ít dữ liệu
- Sử dụng "distillation token" để học từ mô hình CNN (teacher)

### Swin Transformer

- Tính toán self-attention trong các cửa sổ cục bộ (shifted windows)
- Giảm độ phức tạp tính toán, phù hợp cho các tác vụ vision phức tạp
- Kiến trúc phân cấp giống CNN

### PiT (Pooling-based Vision Transformer)

- Kết hợp ưu điểm của CNN (pooling layers) và ViT
- Giảm độ phức tạp tính toán bằng cách giảm độ phân giải qua các lớp

### CrossViT

- Sử dụng nhiều branch với các kích thước patch khác nhau
- Kết hợp thông tin từ các scale khác nhau

## ViT cho Image-to-Image Translation

### VQGAN + CLIP

Kết hợp Vector Quantized GAN (VQGAN) với CLIP (Contrastive Language-Image Pre-training):

1. **VQGAN**: Mã hóa và giải mã ảnh thông qua discrete latent codes
2. **CLIP**: Cung cấp hướng dẫn từ text hoặc ảnh tham chiếu
3. **Quá trình**: Tối ưu latent codes để tối đa hóa sự tương đồng với hướng dẫn

### ViTGAN

- Sử dụng kiến trúc transformer cho cả generator và discriminator
- Cho phép tạo ra ảnh với độ phân giải cao
- Cải thiện chất lượng ảnh so với các GAN truyền thống

### NUWA (Text-to-Image Transformer)

- Mô hình transformer 3D xử lý cả không gian và thời gian
- Có thể tạo ảnh từ text, video từ ảnh, v.v.

## ViT trong Style Transfer

### StyleFormer

- Sử dụng transformer để trích xuất đặc trưng phong cách
- Học biểu diễn phong cách dựa trên attention
- Áp dụng phong cách lên ảnh nội dung một cách linh hoạt

### StyTr² (Style Transformer Squared)

- Sử dụng transformer để phân tách nội dung và phong cách
- Self-attention giúp xác định vùng quan trọng cho việc chuyển phong cách
- Cho phép kiểm soát cân bằng giữa phong cách và nội dung

## Xây dựng mô hình ViT cho chuyển đổi người thật sang anime

### Kiến trúc dựa trên ViT

```python
import torch
import torch.nn as nn
import timm

class ContentEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # Sử dụng ViT pre-trained làm encoder
        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)
        # Chỉ lấy phần encoder
        self.vit.head = nn.Identity()
        
    def forward(self, x):
        return self.vit(x)

class StyleEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # Encoder phong cách riêng biệt
        self.vit = timm.create_model('vit_small_patch16_224', pretrained=True)
        self.vit.head = nn.Identity()
        
    def forward(self, x):
        return self.vit(x)

class TransformerDecoder(nn.Module):
    def __init__(self, dim=768, num_heads=8, num_layers=6):
        super().__init__()
        # Decoder dựa trên Transformer
        decoder_layer = nn.TransformerDecoderLayer(d_model=dim, nhead=num_heads)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        # Đầu ra: tạo ảnh từ features
        self.to_image = nn.Sequential(
            nn.ConvTranspose2d(dim, 512, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )
        
    def forward(self, content, style):
        # Sử dụng content features là input, style features là memory
        features = self.decoder(content, style)
        # Reshape để phù hợp với convolutional layers
        b, n, c = features.shape
        h = w = int(n**0.5)
        features = features.reshape(b, c, h, w)
        # Tạo ảnh
        return self.to_image(features)

class AnimeStyleTransformer(nn.Module):
    def __init__(self):
        super().__init__()
        self.content_encoder = ContentEncoder()
        self.style_encoder = StyleEncoder()
        self.decoder = TransformerDecoder()
        
    def forward(self, content_img, style_img=None):
        content_features = self.content_encoder(content_img)
        
        # Nếu không có style_img, sử dụng style mặc định (anime)
        if style_img is None:
            # Dummy style features có thể thay thế bằng learned style tokens
            style_features = torch.zeros_like(content_features)
        else:
            style_features = self.style_encoder(style_img)
            
        output = self.decoder(content_features, style_features)
        return output
```

### Sử dụng Transfer Learning với ViT

Một cách hiệu quả để áp dụng ViT cho dự án:

1. **Pre-train** mô hình trên tập dữ liệu lớn của cả ảnh người thật và anime
2. **Fine-tune** cho tác vụ chuyển đổi cụ thể
3. **Kết hợp** với các kỹ thuật khác như CycleGAN hoặc Diffusion

### Kết hợp ViT với các phương pháp khác

1. **ViT + CycleGAN**:
   - Sử dụng ViT làm generator trong CycleGAN
   - Tận dụng khả năng nắm bắt mối quan hệ toàn cục của ViT

2. **ViT + Diffusion**:
   - Sử dụng ViT trong U-Net của Diffusion Models
   - Kết hợp ưu điểm của cả Transformer và Diffusion

## Ưu và nhược điểm của hướng tiếp cận ViT

### Ưu điểm

1. **Khả năng nắm bắt cấu trúc toàn cục**: Giúp bảo toàn đặc điểm nhận dạng tốt hơn
2. **Transfer learning hiệu quả**: Có thể tận dụng các mô hình ViT pre-trained
3. **Tính mở rộng**: Dễ dàng kết hợp với text prompts hoặc điều kiện khác
4. **Xử lý dependencies**: Có thể học mối quan hệ giữa các vùng xa nhau trong ảnh

### Nhược điểm

1. **Yêu cầu tính toán cao**: ViT đòi hỏi nhiều tài nguyên tính toán
2. **Cần nhiều dữ liệu**: Hiệu suất kém trên tập dữ liệu nhỏ nếu không pre-train
3. **Kiến trúc phức tạp**: Khó triển khai và tối ưu hơn so với CNN
4. **Ít tài nguyên sẵn có**: Ít mô hình pre-trained dành riêng cho anime so với các lĩnh vực khác

## Tài nguyên để học thêm về ViT

1. **Bài báo gốc**: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
2. **Code mẫu**: timm library cung cấp nhiều triển khai ViT
3. **Tutorials**: PyTorch và TensorFlow đều có hướng dẫn chi tiết về ViT
4. **Pre-trained models**: Available on Hugging Face, timm, và TF Hub