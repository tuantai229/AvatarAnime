# Phase 5: Implementing Approach 3 - Vision Transformer

`src/models/vit/ViT_StyleTransfer_Kaggle.ipynb` 

## 1. Objectives

Using Vision Transformer (ViT) combined with Adaptive Instance Normalization (AdaIN) technique to transform real human portrait photos (content images) into anime style (style images). The main requirements include preserving the identity features of the person in the original image and producing high-quality results with clear characteristics of anime style.

## 2. Execution Environment

* **Platform:** Kaggle Notebook
* **Hardware:** 2 x NVIDIA Tesla T4 GPUs
* **Language:** Python 3.11
* **Main Libraries:**
    * `torch` & `torchvision`: Deep learning framework.
    * `timm`: Library containing pre-trained models (including ViT).
    * `PIL` (Pillow): Image processing.
    * `matplotlib`: Plotting graphs and displaying images.
    * `numpy`: Numerical computing.
    * `tqdm`: Progress bar.
    * `os`, `shutil`: File system operations.

## 3. Data

* **Source:** Kaggle Dataset - [AvatarAnime](https://www.kaggle.com/datasets/tuantai229/avataranime)
* **Path on Kaggle:** `/kaggle/input/avataranime/AvatarAnime/`
    * Content Images (Real People): `CelebA/train`, `CelebA/val`, `CelebA/test`
    * Style Images (Anime): `AnimeFace/train`, `AnimeFace/val`, `AnimeFace/test`
* **Quantity (in notebook):**
    * Train: 800 content, 800 style
    * Validation: 100 content, 100 style
* **Preprocessing & Augmentation:**
    * **Image Size (`IMG_SIZE`):** 224x224
    * **Normalization:** Using ImageNet mean and std values.
    * **Train Transform:** Resize(224), RandomCrop(224), RandomHorizontalFlip, ToTensor, Normalize.
    * **Test/Validation Transform:** Resize(224), CenterCrop(224), ToTensor, Normalize.
    * **`denormalize` Function:** Defined to convert normalized tensor images back to viewable format.
* **Dataset Class:** Using a custom `CustomImageDataset` class to read images directly from the `train`, `val`, `test` folders as the data doesn't have class subdirectory structure.
* **DataLoaders:**
    * `BATCH_SIZE = 8` (Note: When running DataParallel, effective batch size on each GPU is 4).
    * `NUM_WORKERS = 2`.
    * `shuffle=True`, `drop_last=True` for training DataLoader.

## 4. Model Architecture: ViTStyleTransfer

The main model is based on an Encoder-AdaIN-Decoder architecture.

* **Encoder (`vit_encoder`):**
    * Using `timm.create_model('vit_base_patch16_224', pretrained=True)`.
    * The `head` layer (classification) is replaced with `nn.Identity()`.
    * Extracts patch embeddings after Transformer blocks and final LayerNorm.
    * Reshapes output into a spatial feature map: `(B, embed_dim, 14, 14)`.
* **Adaptive Instance Normalization (`adaptive_instance_normalization`):**
    * A separate function that receives content and style feature maps.
    * Calculates mean and std along spatial dimensions for each feature map.
    * Normalizes content features (subtract mean, divide by std).
    * Scales and shifts normalized content features using style features' std and mean.
* **Decoder (`Decoder`):**
    * A CNN network using `nn.ConvTranspose2d` to increase resolution.
    * Architecture: 4 ConvTranspose2d layers (stride=2) to upsample from 14x14 to 224x224, interspersed with `nn.InstanceNorm2d` and `nn.ReLU` (ensuring `inplace=False`).
    * Final layer: `nn.Conv2d` (kernel=3, stride=1, padding=1) to convert channels to 3 (RGB) without changing the 224x224 size.
    * Final activation: `nn.Tanh()` to bring output to [-1, 1] range.
* **Loss Network (`vgg_loss_net`):**
    * Using pre-trained `vgg19` from `torchvision.models` (`VGG19_Weights.DEFAULT`).
    * The entire VGG network is frozen (`requires_grad=False`) and set to `eval()` mode.
    * Features are extracted from specific ReLU layers (indices `[1, 6, 11, 20, 29]`) to compute content and style losses.
* **`forward` Process:**
    1. Encode content and style images using `vit_encoder`.
    2. Apply `adaptive_instance_normalization` to features.
    3. Decode stylized features using `Decoder` to create output image.
* **`calculate_loss` Function:**
    * Extracts VGG features from generated, content, and style images. Features from original content/style images are obtained within `torch.no_grad()` context.
    * **Content Loss:** Calculates `MSELoss` between VGG features (from layer `relu3_1`?, index=2) of generated and content images.
    * **Style Loss:** Calculates `MSELoss` on *mean* and *std* of VGG features (from all selected layers) between generated and style images.
    * Returns separate `content_loss` and `style_loss`.

## 5. Training

* **Device:** Running on 2 Tesla T4 GPUs using `torch.nn.DataParallel`.
    * Base model is initialized and then wrapped with `nn.DataParallel(base_model)`.
* **Optimizer:** `AdamW` with `lr=1e-4`.
* **Hyperparameters:**
    * `NUM_EPOCHS = 50`
    * `LAMBDA_CONTENT = 3.0`
    * `LAMBDA_STYLE = 1.0` (This ratio prioritizes content preservation over strong style application)
* **Training Loop:**
    1. Set model to `train()` mode.
    2. Iterate through batches from `content_loader_train` and `style_loader_train`.
    3. Transfer data to device (`cuda`).
    4. Clear gradients (`optimizer.zero_grad()`).
    5. Forward pass through model to generate `generated_images`.
    6. Calculate loss: Call `model.module.calculate_loss(...)` to access the base model's function when using `DataParallel`.
    7. Calculate `total_loss = LAMBDA_CONTENT * content_loss + LAMBDA_STYLE * style_loss`.
    8. Check for `NaN` in loss.
    9. Calculate gradients (`total_loss.backward()`).
    10. Update weights (`optimizer.step()`).
    11. Log average loss per epoch.
* **Validation:**
    * After each epoch, set model to `eval()` mode.
    * Run inference on a fixed batch from validation set.
    * Denormalize and save result images (content, style, generated) as a grid using `matplotlib`.
* **Save Checkpoint:**
    * Save checkpoint every 5 epochs to `/kaggle/working/ViT_StyleTransfer/checkpoints/`.
    * **Important:** Checkpoint is saved using `model.state_dict()`. Since `model` is a `DataParallel` object, this `state_dict` contains keys with the `module.` prefix.

## 6. Training Results (Based on Logs and Inference Images)

* **Loss:** Loss values (Total, Content, Style) decreased throughout the 50 epochs, indicating model learning. Final loss (Epoch 50) has values of Total ~8.09, Content ~1.42, Style ~3.82. The ratio `LAMBDA_CONTENT=3.0`, `LAMBDA_STYLE=1.0` may explain why Content Loss is relatively low.
* **Image Quality:**
    * **Progression:** From noise in epoch 1, the model quickly learned to maintain facial structure of content images (from epoch 6 onwards).
    * **Final Results (Epoch 50):** Generated images preserve the content and shape of original images fairly well. However, the anime style effect is weak, mainly manifested through changes in color and overall smoothness. Distinctive anime features like large eyes, sharp outlines, characteristic coloring are not clearly present. Images look more like filtered originals rather than complete style transformations.
    * **Convergence:** Quality doesn't seem to improve much after around epoch 15-20, suggesting the model may have converged with current parameters or needs loss adjustment to push style learning further.

## 7. Inference (Using Trained Model)

* The `run_inference` function is defined to load checkpoints and perform style transfer for a new content/style image pair.
* **Loading Checkpoint:**
    * Uses `torch.load(..., weights_only=True)` to load checkpoint.
    * Initializes base `ViTStyleTransfer` model (without `DataParallel`).
    * **Handling `state_dict`:** Code in `run_inference` function is adjusted to check and remove the `module.` prefix from keys in the `state_dict` loaded from checkpoint before loading into the base model. This is necessary because checkpoints were saved from a `DataParallel` object.
* **Process:** Load images -> Transform -> Transfer to device -> Run model (`inference_model(...)` within `torch.no_grad()`) -> Denormalize -> Save result image.
* Notebook successfully ran inference for checkpoints saved every 5 epochs.

## 8. Discussion and Future Development

* **Successes:**
    * Overcame initial noise issues and runtime errors (inplace, attribute error).
    * Successfully trained model on multiple GPUs (2x T4) using `DataParallel`.
    * Model learned to preserve original image content fairly well.
* **Limitations:**
    * Anime style transformation quality doesn't meet requirements (weak style).
* **Planned Improvements:**
    * **Loss Tuning:** Experiment aggressively with increasing `LAMBDA_STYLE` and/or decreasing `LAMBDA_CONTENT`.
    * **Style Loss Method:** Switch to using Gram Matrix instead of mean/std.
    * **VGG Layers:** Experiment with different VGG layers for content and style loss.
    * **TV Loss:** Add Total Variation Loss for image smoothing.
    * **Data:** Check quality and consistency of AnimeFace dataset.
    * **Hyperparameters:** Try lower learning rate, use scheduler.
    * **Training Time:** Train additional epochs after adjustments.