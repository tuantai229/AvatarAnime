# Diffusion Models and Stable Diffusion

## Diffusion Models: Basic Principles

Diffusion Models are a family of generative models leading the field of image creation, using a different approach compared to GANs.

### Diffusion Process

Diffusion Models operate based on two processes:

1. **Forward Diffusion Process**:
   - Gradually adds Gaussian noise to original data through T steps
   - At the final step, original data becomes pure Gaussian noise
   - Formula: x_t = √(1-β_t) * x_{t-1} + √(β_t) * ε

2. **Reverse Diffusion Process**:
   - Learns to remove noise, gradually restoring original data
   - Starts from pure noise, model predicts and removes noise at each step
   - Basic formula: x_{t-1} = f(x_t, t, θ) + σ_t * z

### Learning Objective

Diffusion Models focus on learning to predict the added noise:
- During training, the model learns to predict ε from x_t
- Objective: Minimize MSE between actual noise and predicted noise

Basic loss function:
```
L = E_{x,ε,t}[||ε - ε_θ(x_t, t)||²]
```

### Advantages of Diffusion Models

1. **High quality**: Generates higher quality and more diverse images than GANs
2. **Training stability**: Does not face mode collapse or unstable training issues
3. **Better control**: Allows more detailed control through guidance and conditioning
4. **Easier evaluation**: Can calculate model likelihood

## Stable Diffusion

Stable Diffusion is an advanced diffusion model developed by CompVis, Stability AI, and LAION.

### Stable Diffusion Architecture

The main difference of Stable Diffusion is performing diffusion in latent space rather than directly on images:

1. **Encoder (VAE encoder)**: Compresses images into latent space representation
2. **U-Net with attention**: Performs diffusion process in latent space
3. **Decoder (VAE decoder)**: Decompresses from latent space to image space

### Conditional Generation in Stable Diffusion

Stable Diffusion allows conditioning image creation through:

1. **Text-to-Image**: Using embeddings from CLIP text encoder
2. **Image-to-Image**: Starting from an existing image and performing diffusion
3. **Inpainting/Outpainting**: Changing only part of an image

### Important Concepts

1. **Noise Scheduler**: Controls schedule of adding/removing noise (DDPM, DDIM, ...)
2. **Classifier-Free Guidance**: Technique to enhance condition following
3. **Negative Prompts**: Guiding the model on what NOT to generate

## ControlNet for Stable Diffusion

ControlNet is an important extension for Stable Diffusion, particularly useful for human-to-anime conversion projects.

### How ControlNet Works

- Adds control paths to the Stable Diffusion model
- Allows detailed control of image generation based on conditions like structure, edges, pose, etc.
- Utilizes "zero convolution" architecture to ensure it doesn't interfere with training

### Types of Conditions in ControlNet

1. **Canny Edge**: Control by edges
2. **HED Edge**: Control by edges with more details
3. **Pose/OpenPose**: Control by human skeleton
4. **Depth Map**: Control by depth map
5. **Normal Map**: Control by normal map
6. **Segmentation Map**: Control by segmentation map

### Applying ControlNet for Human-to-Anime Conversion

ControlNet is an excellent choice for this project because it allows:
1. Preserving identity features (through structural control)
2. Maintaining pose and expression from original image
3. Applying anime style consistently

## Fine-tuning Stable Diffusion

### LoRA (Low-Rank Adaptation)

LoRA is an efficient method to fine-tune Stable Diffusion without updating the entire model:

1. **Principle**: Adds low-rank matrices to existing weights
2. **Advantages**: 
   - Requires fewer resources than full fine-tuning
   - Can combine multiple LoRAs
   - Easy to switch between styles

### Textual Inversion

Method for learning new tokens to represent specific styles or objects:

1. **Principle**: Learn embedding for new "word" representing specific concept
2. **Application**: Learn a token representing specific anime style

### DreamBooth

Method to fine-tune for learning about specific subject:

1. **Principle**: Fine-tune entire model with small number of images of subject
2. **Application**: Can be applied to learn how to preserve identity features of specific person

## Implementing Stable Diffusion with ControlNet

### Environment Setup

```python
# Install necessary libraries
!pip install diffusers transformers accelerate ftfy safetensors
!pip install opencv-python controlnet_aux
```

### Basic Model for Image-to-Image with ControlNet

```python
import torch
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image
import numpy as np
import cv2
from PIL import Image

# Load ControlNet model (example: using Canny Edge)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16
)

# Load Stable Diffusion model with ControlNet
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "animefull-final-pruned", # Or other anime model
    controlnet=controlnet,
    torch_dtype=torch.float16,
).to("cuda")

# Prepare control image (from real human photo)
image = load_image("path/to/person.jpg")
image = np.array(image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
control_image = Image.fromarray(image)

# Create anime image
prompt = "high quality anime portrait, detailed anime face, anime style, beautiful anime girl"
negative_prompt = "low quality, bad anatomy, worst quality, blurry"

output = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=control_image,
    num_inference_steps=30,
    guidance_scale=7.5,
).images[0]

output.save("anime_portrait.png")
```

### Combining Multiple ControlNet Conditions

For better results, multiple ControlNet types can be combined:

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, MultiControlNetModel

# Load multiple ControlNet models
canny_controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
pose_controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-openpose", torch_dtype=torch.float16)

# Combine ControlNets
controlnet = MultiControlNetModel([canny_controlnet, pose_controlnet])

# Create pipeline
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "anime-model", controlnet=controlnet, torch_dtype=torch.float16
).to("cuda")

# Prepare control images
canny_image = # Process canny from original image
pose_image = # Process pose from original image

# Create image
output = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=[canny_image, pose_image],
    num_inference_steps=30,
    guidance_scale=7.5,
    control_guidance_start=[0.0, 0.0],
    control_guidance_end=[1.0, 1.0],
).images[0]
```

## Fine-tune Stable Diffusion for Anime Style

### Using LoRA

```python
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
import torch

# Load base Stable Diffusion model
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")

# Load LoRA adapter for anime style
pipe.unet.load_attn_procs(
    "anime-style-lora",
    weight_name="anime_style_lora.safetensors"
)

# Configure scheduler
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

# Use for image-to-image
pipe = StableDiffusionImg2ImgPipeline(**pipe.components)

# Create anime image from real human photo
init_image = load_image("path/to/person.jpg").resize((512, 512))
prompt = "anime style, highly detailed anime character, anime portrait"

image = pipe(
    prompt=prompt,
    image=init_image,
    strength=0.75,  # Change intensity (0-1)
    guidance_scale=7.5,
    num_inference_steps=50
).images[0]
```

## Challenges and Solutions

### Challenges When Using Stable Diffusion

1. **Identity preservation**: Difficult to ensure person in image is still recognizable
   - **Solution**: Use ControlNet + combine with DreamBooth/LoRA fine-tuned for specific person

2. **Style stability**: Ensuring consistent anime style
   - **Solution**: Fine-tune on anime dataset with specific style

3. **Style diversity**: Creating different anime styles
   - **Solution**: Train different LoRAs for different styles

4. **Computational demands**: Stable Diffusion requires powerful GPU
   - **Solution**: Use techniques like model pruning, quantization, or rent cloud GPU