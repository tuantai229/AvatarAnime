# Fine-tuning Stable Diffusion with LoRA

`src/models/diffusion/Stable_Diffusion_with_LoRA_Colab.ipynb` 

## Overview

Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique for large diffusion models like Stable Diffusion. Rather than retraining the entire model, LoRA focuses on training low-rank weight matrices added to the attention layers of the original model, significantly reducing memory requirements and training time.

## Implementation Process

### 1. Environment Setup

First, we install the necessary libraries:
- `diffusers`: Core library for working with Stable Diffusion
- `transformers`: Supporting transformer models
- `accelerate`: Helping accelerate the training process
- `bitsandbytes`: Supporting memory optimization with 8-bit quantization
- Other libraries like `PIL`, `torch`, `xformers`

### 2. Data Preparation

The data preparation process includes:
- Collecting anime images from the source directory (128x128 size)
- Resizing images to 512x512 (necessary for Stable Diffusion)
- Organizing data according to LoRA script requirements:
  - Images placed in directories named with format `[TriggerWord]_[ClassDescription]`
  - Using trigger word `myAnimeStyleToken`
  - Using class name `anime_face`

### 3. LoRA Training

The LoRA training script uses key parameters:
- **Base model**: `runwayml/stable-diffusion-v1-5`
- **Instance prompt**: `myAnimeStyleToken anime_face portrait`
- **Number of epochs**: 5
- **Learning rate**: 1e-4
- **Batch size**: 1 (with gradient accumulation steps = 2)
- **Rank**: 16 (rank of LoRA matrix)
- **Resolution**: 512x512

The training process creates checkpoints every 500 steps and validates after each epoch, generating validation images with the prompt:
`myAnimeStyleToken anime_face girl, high quality`

### 4. Testing with the Trained Model

After training is complete, we conduct various tests:

1. **Text-to-image generation**:
   - Using different prompts to see the model's learned anime style
   - Testing with characters having various features (blue hair, red hair, male/female)
   
2. **Human-to-anime conversion (image-to-image)**:
   - Using `StableDiffusionImg2ImgPipeline`
   - Testing with different `strength` values (0.65, 0.8)
   - Comparing original and converted images

3. **Parameter influence analysis**:
   - Testing with different anime styles (chibi, Ghibli, retro 90s, etc.)
   - Analyzing guidance scale impact (5.0, 7.5, 10.0)
   - Analyzing style application capability with different character features

### 5. Results

The results show:
- The model has learned the anime style and can apply it to various character features
- When using image-to-image with low strength (0.65), the model preserves many features from the original image
- With higher strength (0.8), the anime style is more pronounced but preserves fewer identifying features
- Trigger words play an important role in ensuring consistent style

## Advantages of LoRA Method

1. **Memory efficiency**: Only needs to train a small portion of the model
2. **Short training time**: Takes only about 1-2 hours instead of many days
3. **Reusability**: LoRA weights can be easily shared and combined with other LoRA weights
4. **High customizability**: Can adjust the influence level of LoRA on the results

## Limitations and Potential Improvements

1. **Data-dependent quality**: Requires high-quality and diverse anime data
2. **Identity preservation**: Plain LoRA is not yet optimized for preserving identifying features
3. **Combination potential**: Can be combined with ControlNet to improve identity preservation capability