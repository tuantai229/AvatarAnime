# Stable Diffusion với ControlNet

`src/models/diffusion/Stable_Diffusion_with_Controlnet_Colab.ipynb`

## 1. Giới thiệu

Stable Diffusion kết hợp với ControlNet là một hướng tiếp cận mạnh mẽ cho bài toán chuyển đổi ảnh người sang phong cách hoạt hình anime. Triển khai này cho phép kiểm soát chi tiết quá trình tạo ảnh dựa trên đặc điểm của ảnh gốc, giúp bảo toàn đặc điểm nhận dạng - một trong những thách thức lớn nhất của dự án.

### 1.1 ControlNet là gì?

ControlNet là một mở rộng của Stable Diffusion, cho phép điều khiển quá trình sinh ảnh dựa trên các điều kiện như đường viền (edge), bản đồ độ sâu (depth map), khung xương (pose), hay phân đoạn ngữ nghĩa (segmentation map). Điều này đặc biệt hữu ích cho bài toán chuyển đổi phong cách vì nó giúp giữ lại cấu trúc của ảnh gốc trong khi thay đổi phong cách hình ảnh.

### 1.2 Lợi ích của hướng tiếp cận này

- **Bảo toàn đặc điểm nhận dạng**: ControlNet giúp duy trì cấu trúc khuôn mặt, tư thế và các đặc điểm quan trọng của ảnh gốc
- **Đa dạng phong cách**: Có thể dễ dàng điều chỉnh prompt để tạo nhiều phong cách anime khác nhau
- **Chất lượng cao**: Kết quả thường có độ phân giải và chi tiết tốt hơn so với CycleGAN
- **Linh hoạt**: Có thể sử dụng nhiều loại control condition khác nhau (canny, depth, pose, v.v.)

## 2. Thiết lập môi trường

Để triển khai Stable Diffusion với ControlNet, chúng ta cần cài đặt các thư viện cần thiết:

```python
!pip install diffusers transformers accelerate PilloW opencv-python-headless controlnet_aux matplotlib
```

### 2.1 Import các thư viện

```python
import torch
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from controlnet_aux import CannyDetector  # Sử dụng Canny edge làm ví dụ
from PIL import Image
import numpy as np
import os
import random
import matplotlib.pyplot as plt
```

### 2.2 Cấu hình thiết bị

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")
```

## 3. Tải mô hình ControlNet và Stable Diffusion

### 3.1 Tải ControlNet (Canny)

```python
controlnet_id = "lllyasviel/sd-controlnet-canny"
print(f"Loading ControlNet model: {controlnet_id}")
controlnet = ControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16)
print("ControlNet model loaded.")
```

> **Lưu ý**: Bạn có thể thử các loại ControlNet khác như:
> - `lllyasviel/sd-controlnet-openpose`: Để điều khiển dựa trên tư thế của người
> - `lllyasviel/sd-controlnet-depth`: Để điều khiển dựa trên thông tin độ sâu

### 3.2 Tải Stable Diffusion Pipeline

```python
stable_diffusion_id = "runwayml/stable-diffusion-v1-5"
print(f"Loading Stable Diffusion pipeline with ControlNet support: {stable_diffusion_id}")

# Tải pipeline Img2Img với ControlNet support
pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
    stable_diffusion_id,
    controlnet=controlnet,
    torch_dtype=torch.float16
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_attention_slicing()
pipe.to(device)
print("Img2Img Pipeline ready.")
```

> **Lưu ý**: Có thể thử nghiệm với các checkpoint Stable Diffusion khác như:
> - `emilianJR/AnythinV4.0`: Được fine-tune cho phong cách anime
> - `hakurei/waifu-diffusion`: Chuyên biệt cho anime/manga

### 3.3 Tải bộ xử lý Canny Edge

```python
canny_detector = CannyDetector()
print("Canny detector ready.")
```

## 4. Xử lý ảnh đầu vào

### 4.1 Đọc ảnh và tạo bản đồ Canny Edge

```python
# Đọc ảnh gốc
target_size = 512  # Kích thước tốt cho Stable Diffusion
input_image_pil = load_image(input_image_path)
input_image_pil = input_image_pil.resize((target_size, target_size))

# Chuyển PIL Image sang Numpy array
input_image_np = np.array(input_image_pil)

# Tạo Canny Edge Map
low_threshold = 100
high_threshold = 200
canny_image_np = canny_detector(input_image_np, low_threshold, high_threshold)
canny_image_pil = Image.fromarray(canny_image_np)
```

### 4.2 Hiển thị ảnh gốc và bản đồ Canny

```python
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(input_image_pil)
axes[0].set_title('Ảnh gốc (Resized)')
axes[0].axis('off')

axes[1].imshow(canny_image_pil, cmap='gray')
axes[1].set_title('Bản đồ Canny Edges')
axes[1].axis('off')
plt.show()
```

## 5. Sinh ảnh với Img2Img ControlNet

### 5.1 Định nghĩa prompt và tham số

```python
prompt = "anime style portrait, masterpiece, best quality, sharp focus, detailed illustration, 1boy"
negative_prompt = "photorealistic, real photo, low quality, blurry, noisy, text, watermark, signature, ugly, deformed"

generator_seed = random.randint(0, 1000000)
generator = torch.Generator(device=device).manual_seed(generator_seed)

num_inference_steps = 30
guidance_scale = 9.0
controlnet_conditioning_scale = 0.8

# Tham số quan trọng cho Img2Img
image_strength = 0.5  # Điều chỉnh giá trị này để thay đổi mức độ chuyển đổi (0.0-1.0)
```

### 5.2 Chạy pipeline Img2Img với ControlNet

```python
output_image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=input_image_pil,            # Ảnh gốc
    control_image=canny_image_pil,    # Bản đồ Canny Edge
    strength=image_strength,          # Mức độ thay đổi ảnh gốc
    num_inference_steps=num_inference_steps,
    generator=generator,
    guidance_scale=guidance_scale,
    controlnet_conditioning_scale=controlnet_conditioning_scale,
).images[0]
```

### 5.3 Hiển thị kết quả

```python
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(input_image_pil)
axes[0].set_title('Ảnh gốc (Resized)')
axes[0].axis('off')

axes[1].imshow(canny_image_pil, cmap='gray')
axes[1].set_title('Bản đồ Canny Edges')
axes[1].axis('off')

axes[2].imshow(output_image)
axes[2].set_title('Ảnh kết quả (Anime Style)')
axes[2].axis('off')

plt.show()
```

