{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMQWJ1Lb9zux",
        "outputId": "1f5fc009-692f-46cf-a712-5c1cb662212b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUW4gvnGB9jR",
        "outputId": "20927de2-48d7-4811-d240-fd31dad1f7bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'diffusers'...\n",
            "remote: Enumerating objects: 90220, done.\u001b[K\n",
            "remote: Counting objects: 100% (1213/1213), done.\u001b[K\n",
            "remote: Compressing objects: 100% (600/600), done.\u001b[K\n",
            "remote: Total 90220 (delta 999), reused 614 (delta 613), pack-reused 89007 (from 4)\u001b[K\n",
            "Receiving objects: 100% (90220/90220), 67.58 MiB | 28.21 MiB/s, done.\n",
            "Resolving deltas: 100% (66021/66021), done.\n",
            "/content/diffusers\n",
            "Processing /content/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.34.0.dev0) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.13.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers==0.34.0.dev0) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.34.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.34.0.dev0-py3-none-any.whl size=3602370 sha256=605cdf669f0e54a7f0e663f997df932d9a564498bcc3f1cf6c6f1217fe71ee2b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ey0t1l3f/wheels/30/15/ca/ab6e88c89d6ba7047b3f155894c6c346e7cf06067fd132ae62\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.32.2\n",
            "    Uninstalling diffusers-0.32.2:\n",
            "      Successfully uninstalled diffusers-0.32.2\n",
            "Successfully installed diffusers-0.34.0.dev0\n",
            "/content/diffusers/examples/dreambooth\n",
            "Requirement already satisfied: accelerate>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.51.3)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (6.3.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.18.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (3.1.6)\n",
            "Collecting peft==0.7.0 (from -r requirements.txt (line 7))\n",
            "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (0.30.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 2)) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (0.21.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->-r requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (2025.1.31)\n",
            "Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "Successfully installed peft-0.7.0\n",
            "[Errno 2] No such file or directory: '/content/ # Quay lại thư mục gốc của Colab'\n",
            "/content/diffusers/examples/dreambooth\n"
          ]
        }
      ],
      "source": [
        "# Cài đặt thư viện\n",
        "!pip install -q accelerate bitsandbytes diffusers transformers ftfy Pillow tensorboard torch torchvision xformers einops\n",
        "\n",
        "# Tải mã nguồn diffusers (chứa script training)\n",
        "!git clone https://github.com/huggingface/diffusers\n",
        "%cd diffusers\n",
        "!pip install . # Cài đặt diffusers từ mã nguồn vừa tải\n",
        "%cd examples/dreambooth\n",
        "!pip install -r requirements.txt # Cài các thư viện phụ thuộc cho script\n",
        "%cd /content/ # Quay lại thư mục gốc của Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjBFu95FCMAk",
        "outputId": "40285e67-302a-49db-b428-6daa7668b449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bắt đầu resize ảnh từ: /content/drive/MyDrive/data/AvatarAnime/AnimeFace/train\n",
            "Lưu ảnh đã resize vào: /content/anime_data_512\n",
            "Hoàn thành resize 800 ảnh sang kích thước 512x512.\n",
            "Tạo thư mục instance cho LoRA: /content/anime_lora_data/myAnimeStyleToken_anime_face\n",
            "Sao chép ảnh từ /content/anime_data_512 vào /content/anime_lora_data/myAnimeStyleToken_anime_face\n",
            "Hoàn thành sao chép 800 ảnh vào thư mục huấn luyện LoRA.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# --- 1. ĐỊNH NGHĨA ĐƯỜNG DẪN ---\n",
        "# Đường dẫn gốc trên Google Drive\n",
        "drive_base_path = \"/content/drive/MyDrive\" # Giữ nguyên nếu MyDrive là gốc \n",
        "\n",
        "# Đường dẫn tới dữ liệu gốc (128x128)\n",
        "original_anime_train_dir = os.path.join(drive_base_path, \"data/AvatarAnime/AnimeFace/train\")\n",
        "\n",
        "# Đường dẫn tới thư mục CHỨA dữ liệu đã resize (512x512) trên Colab runtime (cho nhanh)\n",
        "# Tạo thư mục này\n",
        "resized_data_parent_dir = \"/content/anime_data_512\"\n",
        "\n",
        "# Đường dẫn tới thư mục huấn luyện LoRA (chứa thư mục con với ảnh)\n",
        "lora_training_data_dir = \"/content/anime_lora_data\"\n",
        "\n",
        "# Thư mục lưu trữ LoRA weights sau khi train xong\n",
        "lora_output_dir = os.path.join(drive_base_path, \"lora_output/my_anime_style\") # Lưu lại vào Drive\n",
        "\n",
        "# --- 2. TẠO CÁC THƯ MỤC CẦN THIẾT ---\n",
        "os.makedirs(resized_data_parent_dir, exist_ok=True)\n",
        "os.makedirs(lora_training_data_dir, exist_ok=True)\n",
        "os.makedirs(lora_output_dir, exist_ok=True)\n",
        "\n",
        "# --- 3. RESIZE ẢNH TỪ 128x128 SANG 512x512 ---\n",
        "target_size = 512\n",
        "print(f\"Bắt đầu resize ảnh từ: {original_anime_train_dir}\")\n",
        "print(f\"Lưu ảnh đã resize vào: {resized_data_parent_dir}\")\n",
        "\n",
        "# Đếm số lượng ảnh resize thành công\n",
        "resized_count = 0\n",
        "if not os.path.exists(original_anime_train_dir):\n",
        "    print(f\"LỖI: Không tìm thấy thư mục dữ liệu gốc: {original_anime_train_dir}\")\n",
        "else:\n",
        "    for filename in os.listdir(original_anime_train_dir):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
        "            try:\n",
        "                img_path = os.path.join(original_anime_train_dir, filename)\n",
        "                img = Image.open(img_path).convert(\"RGB\") # Mở ảnh và đảm bảo là RGB\n",
        "                img_resized = img.resize((target_size, target_size), Image.LANCZOS) # Resize\n",
        "\n",
        "                # Lưu ảnh đã resize vào thư mục tạm trên Colab\n",
        "                output_path = os.path.join(resized_data_parent_dir, filename)\n",
        "                img_resized.save(output_path)\n",
        "                resized_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Lỗi khi xử lý file {filename}: {e}\")\n",
        "\n",
        "    print(f\"Hoàn thành resize {resized_count} ảnh sang kích thước {target_size}x{target_size}.\")\n",
        "\n",
        "# --- 4. TẠO CẤU TRÚC THƯ MỤC CHO SCRIPT LORA ---\n",
        "# Script LoRA cần ảnh nằm trong thư mục con có tên theo format: [TriggerWord]_[ClassDescription]\n",
        "# Ví dụ: myAnimeStyleToken_anime_face\n",
        "trigger_word = \"myAnimeStyleToken\" # Có thể đổi từ này, nhưng phải là từ lạ, duy nhất\n",
        "class_name = \"anime_face\"\n",
        "instance_folder_name = f\"{trigger_word}_{class_name}\"\n",
        "instance_data_path = os.path.join(lora_training_data_dir, instance_folder_name)\n",
        "\n",
        "print(f\"Tạo thư mục instance cho LoRA: {instance_data_path}\")\n",
        "os.makedirs(instance_data_path, exist_ok=True)\n",
        "\n",
        "# --- 5. SAO CHÉP ẢNH ĐÃ RESIZE VÀO THƯ MỤC INSTANCE ---\n",
        "print(f\"Sao chép ảnh từ {resized_data_parent_dir} vào {instance_data_path}\")\n",
        "copied_count = 0\n",
        "for filename in os.listdir(resized_data_parent_dir):\n",
        "    source_file = os.path.join(resized_data_parent_dir, filename)\n",
        "    destination_file = os.path.join(instance_data_path, filename)\n",
        "    try:\n",
        "        shutil.copyfile(source_file, destination_file)\n",
        "        copied_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi sao chép file {filename}: {e}\")\n",
        "\n",
        "print(f\"Hoàn thành sao chép {copied_count} ảnh vào thư mục huấn luyện LoRA.\")\n",
        "\n",
        "# Dọn dẹp thư mục chứa ảnh resize tạm thời (không bắt buộc, để tiết kiệm dung lượng Colab)\n",
        "# print(f\"Xóa thư mục ảnh resize tạm: {resized_data_parent_dir}\")\n",
        "# shutil.rmtree(resized_data_parent_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6N1ANuDC1Sb",
        "outputId": "d9783f9f-b0e1-44fd-cd8e-fd117b24ff95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- QUÁ TRÌNH HUẤN LUYỆN KẾT THÚC (nếu không có lỗi khác) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-04-28 01:46:19.319070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745804779.339280    5269 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745804779.345516    5269 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-28 01:46:19.366652: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "04/28/2025 01:46:35 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "{'variance_type', 'thresholding', 'sample_max_value', 'prediction_type', 'clip_sample_range', 'timestep_spacing', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
            "{'use_post_quant_conv', 'use_quant_conv', 'shift_factor', 'scaling_factor', 'latents_std', 'mid_block_add_attention', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "{'use_linear_projection', 'dropout', 'time_cond_proj_dim', 'class_embeddings_concat', 'attention_type', 'transformer_layers_per_block', 'addition_embed_type', 'conv_out_kernel', 'addition_time_embed_dim', 'upcast_attention', 'timestep_post_act', 'class_embed_type', 'mid_block_only_cross_attention', 'time_embedding_act_fn', 'cross_attention_norm', 'time_embedding_type', 'time_embedding_dim', 'resnet_skip_time_act', 'only_cross_attention', 'resnet_out_scale_factor', 'num_attention_heads', 'reverse_transformer_layers_per_block', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'projection_class_embeddings_input_dim', 'num_class_embeds', 'mid_block_type', 'dual_cross_attention', 'conv_in_kernel', 'encoder_hid_dim', 'addition_embed_type_num_heads'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "04/28/2025 01:47:41 - INFO - __main__ - ***** Running training *****\n",
            "04/28/2025 01:47:41 - INFO - __main__ -   Num examples = 1\n",
            "04/28/2025 01:47:41 - INFO - __main__ -   Num batches each epoch = 1\n",
            "04/28/2025 01:47:41 - INFO - __main__ -   Num Epochs = 15\n",
            "04/28/2025 01:47:41 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "04/28/2025 01:47:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "04/28/2025 01:47:41 - INFO - __main__ -   Gradient Accumulation steps = 2\n",
            "04/28/2025 01:47:41 - INFO - __main__ -   Total optimization steps = 15\n",
            "\rSteps:   0%|          | 0/15 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora.py\", line 1460, in <module>\n",
            "    main(args)\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora.py\", line 1239, in main\n",
            "    for step, batch in enumerate(train_dataloader):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\", line 566, in __iter__\n",
            "    current_batch = next(dataloader_iter)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 764, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"/content/diffusers/examples/dreambooth/train_dreambooth_lora.py\", line 631, in __getitem__\n",
            "    instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3465, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "IsADirectoryError: [Errno 21] Is a directory: '/content/anime_lora_data/myAnimeStyleToken_anime_face'\n",
            "\rSteps:   0%|          | 0/15 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1194, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 780, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/diffusers/examples/dreambooth/train_dreambooth_lora.py', '--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5', '--instance_data_dir=/content/anime_lora_data', '--output_dir=/content/drive/MyDrive/lora_output/my_anime_style', '--instance_prompt=myAnimeStyleToken anime_face portrait', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=2', '--learning_rate=1e-4', '--lr_scheduler=cosine', '--lr_warmup_steps=100', '--num_train_epochs=15']' returned non-zero exit status 1.\n",
            "bash: line 25: --lora_rank=16: command not found\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Thêm %%bash vào dòng đầu tiên để chạy toàn bộ cell như một shell script\n",
        "\n",
        "# --- ĐỊNH NGHĨA BIẾN MÔI TRƯỜNG CHO SCRIPT ---\n",
        "export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\" # Base model\n",
        "export INSTANCE_DIR=\"/content/anime_lora_data\" # Thư mục chứa thư mục [trigger]_[class] với ảnh 512x512\n",
        "export OUTPUT_DIR=\"/content/drive/MyDrive/lora_output/my_anime_style\" # Nơi lưu LoRA weights trên Drive\n",
        "export CLASS_NAME=\"anime_face\" # Tên class (phần sau trigger word trong tên thư mục)\n",
        "export TRIGGER_WORD=\"myAnimeStyleToken\" # Từ kích hoạt\n",
        "\n",
        "# --- CHẠY SCRIPT HUẤN LUYỆN ---\n",
        "# Lệnh accelerate launch vẫn giữ nguyên, CHỈ XÓA dòng --max_train_steps\n",
        "accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --instance_data_dir=$INSTANCE_DIR \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --instance_prompt=\"$TRIGGER_WORD $CLASS_NAME portrait\" `# Prompt dùng trong training` \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 `# Batch size = 1 hoặc 2 là an toàn cho Colab T4` \\\n",
        "  --gradient_accumulation_steps=2 `# Mô phỏng batch size lớn hơn (1*2=2)` \\\n",
        "  --learning_rate=1e-4 `# Learning rate` \\\n",
        "  --lr_scheduler=\"cosine\" \\\n",
        "  --lr_warmup_steps=100 \\\n",
        "  --num_train_epochs=15 `# Số epochs (sẽ dùng cái này để dừng)` \\\n",
        "  # <<< DÒNG --max_train_steps=None ĐÃ BỊ XÓA >>> \\\n",
        "  --lora_rank=16 `# Rank của LoRA (thử 8, 16, 32)` \\\n",
        "  --seed=42 \\\n",
        "  --mixed_precision=\"fp16\" `# Dùng float16` \\\n",
        "  --use_8bit_adam `# Dùng optimizer 8bit` \\\n",
        "  --gradient_checkpointing `# Tiết kiệm VRAM bằng gradient checkpointing` \\\n",
        "  --enable_xformers_memory_efficient_attention `# Dùng xformers` \\\n",
        "  --checkpointing_steps=500 `# Lưu checkpoint mỗi 500 steps` \\\n",
        "  --validation_prompt=\"$TRIGGER_WORD $CLASS_NAME girl, high quality\" `# Prompt để tạo ảnh test trong lúc train` \\\n",
        "  --validation_epochs=2 `# Tạo ảnh test mỗi 2 epoch` \\\n",
        "  --report_to=\"tensorboard\" # Báo cáo log cho tensorboard\n",
        "\n",
        "echo \"--- QUÁ TRÌNH HUẤN LUYỆN KẾT THÚC (nếu không có lỗi khác) ---\" # echo thay vì print trong bash"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
